---
layout: article
title: Google's Tensor Processing Unit
key: 20181204
lang: en
tags: [domain-specific hardware, neural network]
modify_date: 2018-12-5
---

![TPUchip](https://blog-1256135234.cos.ap-chengdu.myqcloud.com/TPU/Mjg4NDc4NQ.jpeg)

Google has published its paper describing about TPU in [In-Datacenter Performance Analysis of a Tensor Processing Unit](https://arxiv.org/ftp/arxiv/papers/1704/1704.04760.pdf). According to this paper, Google's applications are mostly user-facing, which leads to rigid response-time limits, and thus **TPU focuses on reducing latency**. One intersting thing about Google's demand survey in this paper is that the most widely discussed neural network -- CNN, just occupies 5% of Google's datacenter workload.

<!--more-->

### TPU Architecture
![TPUarchitecture](https://blog-1256135234.cos.ap-chengdu.myqcloud.com/TPU/google-ai-chip-100718206-large.jpg)

*	The TPU serves as a coprocessor on the PCIe I/O bus instead of being integrated with a CPU. And it receives instructions from the host server. 
* The internal blocks are typically connected together by 256-byte-wide **paths**.
* **Matrix Multiply Unit** contains 256*256 MACs that can perform 8-bit multiply-and-adds on signed or unsigned integers. It produces one 256-element partial sum per clock cycle. The matrix unit holds one 64KiB tile of weights plus one for double-buffering (to hide the 256 cycles it takes to shift a tile in). Note that this unit is designed for dense matrices and sparse architectural support was omitted for time-to-deploy reasons.
* **Accumulators**: 4MiB of 256-element, 32-bit. 4MiB (4096) are picked that the operations per byte need to reach peak performance is 2048(actually 1350, rounded up to 2048?) and then duplicated it for double buffering.
* **Weight FIFO**: On-chip FIFO for staging the weights for the matrix unit. The weight FIFO is four tiles deep.
* **Weight Memory**: Off-chip 8 GiB DRAM supporting simultaneously reading from multiple active models.
* **Unified Buffer**: On-chip, 24 MiB, which serves as inputs to the Matrix Unit.

### TPU ISA
* TPU instructions follow the CISC tradition due to the relatively slow PCIe bus.
* **Read_Host_Memory** reads data from the CPU host memory into the Unified Buffer.
* **Read_Weights** reads weights from Weight Memory into the Weight FIFO as input to the Matrix Unit.
* **MatrixMultiply/Convolve** causes the Matrix Unit to perform a matrix multiply or a convolution from Unified Buffer into the Accumulators. A matrix operation takes a variable-sized B*256 input, multiplies it by a 256*256 constant weight input, and produces a B*256 output, taking B pipelined cycles to complete.
* **Activate** performs the nonlinear function of the artificial neuron, with options for ReLU, Sigmoid, and so on. Its inputs are the Accumulators, and its output is the Unified Buffer. It can also perform the pooling operations needed for convolutions using the dedicated hardware on the die, as it is connected to nonlinear function logic.
* **Write_Host_Memory** writes data from the Unified Buffer into the CPU host memory.

### TPU Microarchitecture
* The CISC MatrixMultiply instructions is 12 bytes, of which 3 are Unified Buffer address; 2 are accumulator address; 4 are length (sometimes 2 dimensions for convolutions); and the rest are opcode and flags.
* TPU uses a 4-stage pipeline for these CISC instructions, where each instruction executes in a separate stage. The plan was to hide the execution of the other instructions by overlapping their execution with the MatrixMultiply instruction. Toward that end, the Read_Weights instruction follows the decoupled-access/execute philosophy, in that it can complete after sending its address but before the weight is fetched from Weight memory. The matrix unit will stall if the input activation or weight data is not ready.
* CISC instructions of TPU can occupy a station for thousands of clock cycles, unlike the traditional RISC pipeline with one clock cycle per stage.
* The matrix unit uses systolic execution to save energy by reducing reads and writes of the Unified Buffer. In Google's paper its mechanism was just described as "It relies on data from different directions arriving at cells in an array at regular intervals where they are combined", and it's hard to understand for freshers like me. To know more about systolic architectures, please refer to [Why systolic architectures?](http://www.eecs.harvard.edu/~htk/publication/1982-kung-why-systolic-architecture.pdf) published in 1982 by H.T.Kung. Briefly saying, itâ€™s an architecture designed to balance computing with I/O in a simple and regular design. The computing pattern are illustrated by the following figure:

![systolicarray](https://blog-1256135234.cos.ap-chengdu.myqcloud.com/TPU/v2-d878144bcca599494ee6007a7507c2fa_r.jpg){:height="70%" width="70%"}

The upper half of this figure describe the traditional computing model: A PE reads data from memory, processes and writes it back. Due to the fact that data reading/storing is much slowe than processing, its throughput is limited by its memory access ability. So the author figured out a way called systolic array to make data processed by multiple PEs. Specifically, the first data element flows into the first PE, after processing, it will be ransmitted to the next PE, meanwhile the second element will be passed to the first PE, and in this way, the systolic array reuses data and achieves a high throughout by consuming little memory bandwidth.

Convolution is an ideal computation pattern that takes full advantage of systolic array.

![convolutiondefinition](https://blog-1256135234.cos.ap-chengdu.myqcloud.com/TPU/v2-858ad056658e5a9bd72314411d740d84_hd.png)

Below is an example, given the input X broadcasted to each processing element, the weight value W prestored in each PE, after 3 clock cycles, the result Y will constantly be computed following the formula.

![vectorsystoliccomputation](https://blog-1256135234.cos.ap-chengdu.myqcloud.com/TPU/v2-8d02652c3a1bd57bf93979ea4583a453_hd.png){:height="70%" width="70%"}

Extend array to 2-dim tensor, the systolic computing can be done in the following way:

![2dmatrixsystoliccomputation](https://blog-1256135234.cos.ap-chengdu.myqcloud.com/TPU/ezgif.com-gif-maker.gif)

Recur to TPU's matrix unit, as the below figure shows, the data flows in from the left, and the weights are loaded from the top.

![systolicdataflow](https://blog-1256135234.cos.ap-chengdu.myqcloud.com/TPU/systolic_data_flow.PNG){:height="70%" width="70%"}

> The weights are preloaded, and take effect with the advancing wave alongside the first data of a new block. Control and data are pipelined to give the illusion that the 256 inputs are read at once, and that they instantly update one location of each of 256 accumulators.

It should be notified that the input data and weight have been transformed in the Systolic Data Setup area to be compatible with the systolic computation, which means the both may not have the same shape and order.

### TPU Software Stack
* The portion of the application run on the TPU is typically **written in TensorFlow** and is compiled into an API that can run on GPUs or TPUs. Like GPUs, the TPU stack is split into a **User Driver** and a **Kernel Driver**. The kernel Driver handles only memory management and interrupts.
* The User Space Driver sets up and controls TPU execution, **reformats data into TPU order**, translates API call into TPU instructions, and turns them into an application binary. The User Space Driver compiles a model the first time it is evaluated, caching the program image and writing the weight image into the TPU's weight memory; the second and following evaluations run at full speed. Computation is often done one layer at a time.
* Google doesn't give much detail on the mechanism of TPU's software stack.





