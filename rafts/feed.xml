<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2018-09-30T21:18:40+08:00</updated><id>http://localhost:4000/</id><title type="html">Zio Lee’s Blog</title><subtitle>Research, review and essay.
</subtitle><author><name>Zio Lee</name><email>pingziwalk@gmail.com</email></author><entry xml:lang="en"><title type="html">Abstracts reading – ISCA 2018</title><link href="http://localhost:4000/2018/09/16/ISCA-2018.html" rel="alternate" type="text/html" title="Abstracts reading -- ISCA 2018" /><published>2018-09-16T00:00:00+08:00</published><updated>2018-09-16T00:00:00+08:00</updated><id>http://localhost:4000/2018/09/16/ISCA-2018</id><content type="html" xml:base="http://localhost:4000/2018/09/16/ISCA-2018.html">&lt;h2 id=&quot;the-45th-international-symposium-on-computer-architecture&quot;&gt;The 45th International Symposium on Computer Architecture&lt;/h2&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;session-1a-clouds--datacenters&quot;&gt;Session 1A: Clouds &amp;amp; Datacenters&lt;/h3&gt;

&lt;h4 id=&quot;a-configurable-cloud-scale-dnn-processor-for-real-time-ai-star&quot;&gt;A Configurable Cloud-Scale DNN Processor for Real-Time AI :star:&lt;/h4&gt;

&lt;p&gt;Interactive AI-powered services require low-latency evaluation of deep neural network (DNN) models-aka ““real-time AI””. The growing demand for computationally expensive, state-of-the-art DNNs, coupled with diminishing performance gains of general-purpose architectures, has fueled an explosion of specialized Neural Processing Units (NPUs). NPUs for interactive services should satisfy two requirements: (1) execution of DNN models with low latency, high throughput, and high efficiency, and (2) flexibility to accommodate evolving state-of-the-art models (e.g., RNNs, CNNs, MLPs) without costly silicon updates. This paper describes the NPU architecture for Project Brainwave, a production-scale system for real-time AI. The Brainwave NPU achieves more than an order of magnitude improvement in latency and throughput over state-of-the-art GPUs on large RNNs at a batch size of 1. The NPU attains this performance using a single-threaded SIMD ISA paired with a distributed microarchitecture capable of dispatching over 7M operations from a single instruction. The spatially distributed microarchitecture, scaled up to 96,000 multiply-accumulate units, is supported by hierarchical instruction decoders and schedulers coupled with thousands of independently addressable high-bandwidth on-chip memories, and can transparently exploit many levels of fine-grain SIMD parallelism. When targeting an FPGA, microarchitectural parameters such as native datapaths and numerical precision can be “synthesis specialized” to models at compile time, enabling atypically high FPGA performance competitive with hardened NPUs. When running on an Intel Stratix 10 280 FPGA, the Brainwave NPU achieves performance ranging from ten to over thirty-five teraflops, with no batching, on large, memory-intensive RNNs.&lt;/p&gt;

&lt;h4 id=&quot;virtual-melting-temperature-managing-server-load-to-minimize-cooling-overhead-with-phase-change-materials&quot;&gt;Virtual Melting Temperature: Managing Server Load to Minimize Cooling Overhead with Phase Change materials&lt;/h4&gt;

&lt;p&gt;As the power density and power consumption of large scale datacenters continue to grow, the challenges of removing heat from these datacenters and keeping them cool is an increasingly urgent and costly. With the largest datacenters now exceeding over 200 MW of power, the cooling systems that prevent overheating cost on the order of tens of millions of dollars. Prior work proposed to deploy phase change materials (PCM) and use Thermal Time Shifting (TTS) to reshape the thermal load of a datacenter by storing heat during peak hours of high utilization and releasing it during off hours when utilization is low, enabling a smaller cooling system to handle the same peak load. The peak cooling load reduction enabled by TTS is greatly beneficial, however TTS is a passive system that cannot handle many workload mixtures or adapt to changing load or environmental characteristics. In this work we propose VMT, a thermal aware job placement technique that adds an active, tunable component to enable greater control over datacenter thermal output. We propose two different job placement algorithms for VMT and perform a scale out study of VMT in a simulated server cluster. We provide analysis of the use cases and trade-offs of each algorithm, and show that VMT reduces peak cooling load by up to 12.8% to provide over two million dollars in cost savings when a smaller cooling system is installed, or allows for over 7,000 additional servers to be added in scenarios where TTS is ineffective.&lt;/p&gt;

&lt;h4 id=&quot;firesim-fpga-accelerated-cycle-exact-scale-out-system-simulation-in-the-public-cloud&quot;&gt;FireSim: FPGA-Accelerated Cycle-Exact Scale-Out System Simulation in the Public Cloud&lt;/h4&gt;

&lt;p&gt;We present FireSim, an open-source simulation platform that enables cycle-exact microarchitectural simulation of large scale-out clusters by combining FPGA-accelerated simulation of silicon-proven RTL designs with a scalable, distributed network simulation. Unlike prior FPGA-accelerated simulation tools, FireSim runs on Amazon EC2 F1, a public cloud FPGA platform, which greatly improves usability, provides elasticity, and lowers the cost of large-scale FPGA-based experiments. We describe the design and implementation of FireSim and show how it can provide sufficient performance to run modern applications at scale, to enable true hardware-software co-design. As an example, we demonstrate automatically generating and deploying a target cluster of 1,024 3.2 GHz quad-core server nodes, each with 16 GB of DRAM, interconnected by a 200 Gbit/s network with 2 microsecond latency, which simulates at a 3.4 MHz processor clock rate (less than 1,000x slowdown over real-time). In aggregate, this FireSim instantiation simulates 4,096 cores and 16 TB of memory, runs ~ 14 billion instructions per second, and harnesses 12.8 million dollars worth of FPGAs-at a total cost of only ~ $100 per simulation hour to the user. We present several examples to show how FireSim can be used to explore various research directions in warehouse-scale machine design, including modeling networks with high-bandwidth and low-latency, integrating arbitrary RTL designs for a variety of commodity and specialized datacenter nodes, and modeling a variety of datacenter organizations, as well as reusing the scale-out FireSim infrastructure to enable fast, massively parallel cycle-exact single-node microarchitectural experimentation.&lt;/p&gt;

&lt;h3 id=&quot;session-1b-accelerators-for-emerging-apps&quot;&gt;Session 1B: Accelerators for Emerging Apps&lt;/h3&gt;

&lt;h4 id=&quot;promise-an-end-to-end-design-of-a-programmable-mixed-signal-accelerator-for-machine-learning-algorithms&quot;&gt;PROMISE: An End-to-End Design of a Programmable Mixed-Signal Accelerator for Machine-Learning Algorithms&lt;/h4&gt;

&lt;p&gt;Analog/mixed-signal machine learning (ML) accelerators exploit the unique computing capability of analog/mixed-signal circuits and inherent error tolerance of ML algorithms to obtain higher energy efficiencies than digital ML accelerators. Unfortunately, these analog/mixed-signal ML accelerators lack programmability, and even instruction set interfaces, to support diverse ML algorithms or to enable essential software control over the energy-vs-accuracy tradeoffs. We propose PROMISE, the first end-to-end design of a PROgrammable MIxed-Signal accElerator from Instruction Set Architecture (ISA) to high-level language compiler for acceleration of diverse ML algorithms. We first identify prevalent operations in widely-used ML algorithms and key constraints in supporting these operations for a programmable mixed-signal accelerator. Second, based on that analysis, we propose an ISA with a PROMISE architecture built with silicon-validated components for mixed-signal operations. Third, we develop a compiler that can take a ML algorithm described in a high-level programming language (Julia) and generate PROMISE code, with an IR design that is both language-neutral and abstracts away unnecessary hardware details. Fourth, we show how the compiler can map an application-level error tolerance specification for neural network applications down to low-level hardware parameters (swing voltages for each application Task) to minimize energy consumption. Our experiments show that PROMISE can accelerate diverse ML algorithms with energy efficiency competitive even with fixed-function digital ASICs for specific ML algorithms, and the compiler optimization achieves significant additional energy savings even for only 1% extra errors.&lt;/p&gt;

&lt;h4 id=&quot;computation-reuse-in-dnns-by-exploiting-input-similarity-star&quot;&gt;Computation Reuse in DNNs by Exploiting Input Similarity :star:&lt;/h4&gt;

&lt;p&gt;In recent years, Deep Neural Networks (DNNs) have achieved tremendous success for diverse problems such as classification and decision making. Efficient support for DNNs on CPUs, GPUs and accelerators has become a prolific area of research, resulting in a plethora of techniques for energy-efficient DNN inference. However, previous proposals focus on a single execution of a DNN. Popular applications, such as speech recognition or video classification, require multiple back-to-back executions of a DNN to process a sequence of inputs (e.g., audio frames, images). In this paper, we show that consecutive inputs exhibit a high degree of similarity, causing the inputs/outputs of the different layers to be extremely similar for successive frames of speech or images of a video. Based on this observation, we propose a technique to reuse some results of the previous execution, instead of computing the entire DNN. Computations related to inputs with negligible changes can be avoided with minor impact on accuracy, saving a large percentage of computations and memory accesses. We propose an implementation of our reuse-based inference scheme on top of a state-of-the-art DNN accelerator. Results show that, on average, more than 60% of the inputs of any neural network layer tested exhibit negligible changes with respect to the previous execution. Avoiding the memory accesses and computations for these inputs results in 63% energy savings on average.&lt;/p&gt;

&lt;h4 id=&quot;genax-a-genome-sequencing-accelerator&quot;&gt;GenAx: A Genome Sequencing Accelerator&lt;/h4&gt;

&lt;p&gt;Genomics can transform health-care through precision medicine. Plummeting sequencing costs would soon make genome testing affordable to the masses. Compute efficiency, however, has to improve by orders of magnitude to sequence and analyze the raw genome data. Sequencing software used today can take several hundreds to thousands of CPU hours to align reads to a reference sequence. This paper presents GenAx, an accelerator for read alignment, a time-consuming step in genome sequencing. It consists of a seeding and seed-extension accelerator. The latter is based on an innovative automata design that was designed from the ground-up to enable hardware acceleration. Unlike conventional Levenshtein automata, it is string independent and scales quadratically with edit distance, instead of string length. It supports critical features commonly used in sequencing such as affine gap scoring and traceback. GenAx provides a throughput of 4,058K reads/s for Illumina 101 bp reads. GenAx achieves 31.7× speedup over the standard BWA-MEM sequence aligner running on a 56-thread dualsocket 14-core Xeon E5 server processor, while reducing power consumption by 12× and area by 5.6×.&lt;/p&gt;

&lt;h3 id=&quot;session-2a-prefetching&quot;&gt;Session 2A: Prefetching&lt;/h3&gt;

&lt;h4 id=&quot;division-of-labor-a-more-effective-approach-to-prefetching&quot;&gt;Division of Labor: A More Effective Approach to Prefetching&lt;/h4&gt;

&lt;p&gt;Prefetching is a central component in most microarchitectures. Many different algorithms have been proposed with varying degrees of complexity and effectiveness. There are inherent tradeoffs among various metrics especially when we try to exploit both simpler access patterns and more complex ones simultaneously. Hypothetically, therefore, it is better to have collaboration of sub-components each specialized in exploiting a different access pattern than to have a monolithic design trying to have a similar prefetching scope. In this paper, we present some empirical evidence.We use a few components dedicated for some simple patterns such as canonical strided accesses. We show that a composite prefetcher with these components can significantly out-perform state-of-the-art prefetchers. But more importantly, the composite prefetcher achieves better performance through a more limited prefetching scope while attaining a much higher accuracy. This suggests that the design can be more readily expanded with additional components targeting other patterns.&lt;/p&gt;

&lt;h4 id=&quot;critically-aware-tiered-cache-hierachy-a-fundamental-relook-at-multi-level-cache-hierarchies&quot;&gt;Critically Aware Tiered Cache Hierachy: A fundamental relook at multi-level cache hierarchies&lt;/h4&gt;

&lt;p&gt;On-die caches are a popular method to help hide the main memory latency. However, it is difficult to build large caches without substantially increasing their access latency, which in turn hurts performance. To overcome this difficulty, on-die caches are typically built as a multi-level cache hierarchy. One such popular hierarchy that has been adopted by modern microprocessors is the three level cache hierarchy. Building a three level cache hierarchy enables a low average hit latency since most requests are serviced from faster inner level caches. This has motivated recent microprocessors to deploy large level-2 (L2) caches that can help further reduce the average hit latency.
In this paper, we do a fundamental analysis of the popular three level cache hierarchy and understand its performance delivery using program criticality. Through our detailed analysis we show that the current trend of increasing L2 cache sizes to reduce average hit latency is, in fact, an inefficient design choice. We instead propose Criticality Aware Tiered Cache Hierarchy (CATCH) that utilizes an accurate detection of program criticality in hardware and using a novel set of inter-cache prefetchers ensures that on-die data accesses that lie on the critical path of execution are served at the latency of the fastest level-1 (L1) cache. The last level cache (LLC) serves the purpose of reducing slow memory accesses, thereby making the large L2 cache redundant for most applications. The area saved by eliminating the L2 cache can then be used to create more efficient processor configurations. Our simulation results show that CATCH outperforms the three level cache hierarchy with a large 1 MB L2 and exclusive LLC by an average of 8.4%, and a baseline with 256 KB L2 and inclusive LLC by 10.3%. We also show that CATCH enables a powerful framework to explore broad chip-level area, performance and power tradeoffs in cache hierarchy design. Supported by CATCH, we evaluate radical architecture directions such as eliminating the L2 altogether and show that such architectures can yield 4.5% performance gain over the baseline at nearly 30% lesser area or improve the performance by 7.3% at the same area while reducing energy consumption by 11%.&lt;/p&gt;

&lt;h4 id=&quot;rethinking-beladys-algorithm-to-accommodate-prefetching&quot;&gt;Rethinking Belady’s Algorithm to Accommodate Prefetching&lt;/h4&gt;

&lt;p&gt;This paper shows that in the presence of data prefetchers, cache replacement policies are faced with a large unexplored design space. In particular, we observe that while Belady’s MIN algorithm minimizes the total number of cache misses—including those for prefetched lines—it does not minimize the number of demand misses. To address this shortcoming, we introduce Demand-MIN, a variant of Belady’s algorithm that minimizes the number of demand misses at the cost of increased prefetcher traffic. Together, MIN and Demand-MIN define the boundaries of an important design space, with many intermediate points lying between them.
To reason about this design space, we introduce a simple
conceptual framework, which we use to define a new cache
replacement policy called Harmony. Our empirical evaluation shows that for a mix of SPEC 2006 benchmarks running on a 4-core system with a stride prefetcher, Harmony improves IPC by 7.7% over an LRU baseline, compared to 3.7% for the previous state-of-the-art. On an 8-core system, Harmony improves IPC by 9.4% compared to 4.4% for the previous state-of-the-art.&lt;/p&gt;

&lt;h3 id=&quot;session-2b-languages--models&quot;&gt;Session 2B: Languages &amp;amp; Models&lt;/h3&gt;

&lt;h4 id=&quot;constructing-a-weak-memory-model&quot;&gt;Constructing a Weak Memory Model&lt;/h4&gt;

&lt;p&gt;Weak memory models are a consequence of the desire on part of architects to preserve all the uniprocessor optimizations while building a shared memory multiprocessor. The efforts to formalize weak memory models of ARM and POWER over the last decades are mostly empirical – they try to capture empirically observed behaviors – and end up providing no insight into the inherent nature of weak memory models. This paper takes a constructive approach to find a common base for weak memory models: we explore what a weak memory would look like if we constructed it with the explicit goal of preserving all the uniprocessor optimizations. We will disallow some optimizations which break a programmer’s intuition in highly unexpected ways. The constructed model, which we call General Atomic Memory Model (GAM), allows all four load/store reorderings. We give the construction procedure of GAM, and provide insights which are used to define its operational and axiomatic semantics. Though no attempt is made to match GAM to any existing weak memory model, we show by simulation that GAM has comparable performance with other models. No deep knowledge of memory models is needed to read this paper.&lt;/p&gt;

&lt;h4 id=&quot;a-hardware-accelerator-for-tracing-garbage-collection-star&quot;&gt;A Hardware Accelerator for Tracing Garbage Collection :star:&lt;/h4&gt;

&lt;p&gt;A large number of workloads are written in garbage-collected languages. These applications spend up to 10- 35% of their CPU cycles on GC, and these numbers increase further for pause-free concurrent collectors. As this amounts to a significant fraction of resources in scenarios ranging from data centers to mobile devices, reducing the cost of GC would improve the efficiency of a wide range of workloads. We propose to decrease these overheads by moving GC into a small hardware accelerator that is located close to the memory controller and performs GC more efficiently than a CPU. We first show a general design of such a GC accelerator and describe how it can be integrated into both stop-the-world and pause-free garbage collectors. We then demonstrate an end-toend RTL prototype of this design, integrated into a RocketChip RISC-V System-on-Chip (SoC) executing full Java benchmarks within JikesRVM running under Linux on FPGAs. Our prototype performs the mark phase of a tracing GC at 4.2x the performance of an in-order CPU, at just 18.5% the area (an amount equivalent to 64KB of SRAM). By prototyping our design in a real system, we show that our accelerator can be adopted without invasive changes to the SoC, and estimate its performance, area and energy.&lt;/p&gt;

&lt;h4 id=&quot;charm-a-language-for-closed-form-high-level-architecture-modeling-star&quot;&gt;Charm: A Language for Closed-form High-level Architecture Modeling :star:&lt;/h4&gt;

&lt;p&gt;As computer architecture continues to expand beyond software-agnostic microarchitecture to data center organization, reconfigurable logic, heterogeneous systems, applicationspecific logic, and even radically different technologies such as quantum computing, detailed cycle-level simulation is no longer presupposed. Exploring designs under such complex interacting relationships (e.g., performance, energy, thermal, cost, voltage, frequency, cooling energy, leakage, etc.) calls for a more integrative but higher-level approach. We propose Charm, a domain specific language supporting Closed-form High-level ARchitecture Modeling. Charm enables mathematical representations of mutually dependent architectural relationships to be specified, composed, checked, evaluated and reused. The language is interpreted through a combination of symbolic evaluation (e.g., restructuring) and compiler techniques (e.g., memoization and invariant hoisting), generating executable evaluation functions and optimized analysis procedures. Further supporting reuse, a type system constrains architectural quantities and ensures models operate only in a validated domain. Through two case studies, we demonstrate that Charm allows one to define highlevel architecture models concisely, maximize reusability, capture unreasonable assumptions and inputs, and significantly speedup design space exploration.&lt;/p&gt;

&lt;h3 id=&quot;session-3a-virtual-memory&quot;&gt;Session 3A: Virtual Memory&lt;/h3&gt;

&lt;h4 id=&quot;get-out-of-the-valley-power-efficient-address-mapping-for-gpus-star&quot;&gt;Get Out of the Valley: Power-Efficient Address Mapping for GPUs :star:&lt;/h4&gt;

&lt;p&gt;GPU memory systems adopt a multi-dimensional hardware structure to provide the bandwidth necessary to support 100s to 1000s of concurrent threads. On the software side, GPU-compute workloads also use multi-dimensional structures to organize the threads. We observe that these structures can combine unfavorably and create significant resource imbalance in the memory subsystem — causing low performance and poor power-efficiency. The key issue is that it is highly applicationdependent which memory address bits exhibit high variability. To solve this problem, we first provide an entropy analysis approach tailored for the highly concurrent memory request behavior in GPU-compute workloads. Our window-based entropy metric captures the information content of each address bit of the memory requests that are likely to co-exist in the memory system at runtime. Using this metric, we find that GPU-compute workloads exhibit entropy valleys distributed throughout the lower order address bits. This indicates that efficient GPU-address mapping schemes need to harvest entropy from broad address-bit ranges and concentrate the entropy into the bits used for channel and bank selection in the memory subsystem. This insight leads us to propose the Page Address Entropy (PAE) mapping scheme which concentrates the entropy of the row, channel and bank bits of the input address into the bank and channel bits of the output address. PAE maps straightforwardly to hardware and can be implemented with a tree of XOR-gates. PAE improves performance by 1.31× and power-efficiency by 1.25× compared to state-of-the-art permutation-based address mapping.&lt;/p&gt;

&lt;h4 id=&quot;scheduling-page-table-walks-for-irregular-gpu-applications-star&quot;&gt;Scheduling page table walks for irregular GPU applications :star:&lt;/h4&gt;

&lt;p&gt;Recent studies on commercial hardware demonstrated that irregular GPU applications can bottleneck on virtual-to-physical address translations. In this work, we explore ways to reduce address translation overheads for such applications. We discover that the order of servicing GPU’s address translation requests (specifically, page table walks) plays a key role in determining the amount of translation overhead experienced by an application. We find that different SIMD instructions executed by an application require vastly different amounts of work to service their address translation needs, primarily depending upon the number of distinct pages they access. We show that better forward progress is achieved by prioritizing translation requests from the instructions that require less work to service their address translation needs. Further, in the GPU’s Single-Instruction-Multiple-Thread (SIMT) execution paradigm, all threads that execute in lockstep (wavefront) need to finish operating on their respective data elements (and thus, finish their address translations) before the execution moves ahead. Thus, batching walk requests originating from the same SIMD instruction could reduce unnecessary stalls. We demonstrate that the reordering of translation requests based on the above principles improves the performance of several irregular GPU applications by 30% on average.&lt;/p&gt;

&lt;h4 id=&quot;seesaw-using-superpages-to-improve-vipt-caches&quot;&gt;SEESAW: Using Superpages to Improve VIPT Caches&lt;/h4&gt;

&lt;h4 id=&quot;a-case-for-richer-cross-layer-abstractions-bridging-the-semantic-gap-with-expressive-memory&quot;&gt;A Case for Richer Cross-layer Abstractions: Bridging the Semantic Gap with Expressive Memory&lt;/h4&gt;

&lt;p&gt;This paper makes a case for a new cross-layer interface, Expressive Memory (XMem), to communicate higher-level program semantics from the application to the system software and hardware architecture. XMem provides (i) a flexible and extensible abstraction, called an Atom, enabling the application to express key program semantics in terms of how the program accesses data and the attributes of the data itself, and (ii) new cross-layer interfaces to make the expressed higher-level information available to the underlying OS and architecture. By providing key information that is otherwise unavailable, XMem exposes a new, rich view of the program data to the OS and the different architectural components that optimize memory system performance (e.g., caches, memory controllers). By bridging the semantic gap between the application and the underlying memory resources, XMem provides two key benefits. First, it enables architectural/system-level techniques to leverage key program semantics that are challenging to predict or infer. Second, it improves the efficacy and portability of software optimizations by alleviating the need to tune code for specific hardware resources (e.g., cache space). While XMem is designed to enhance and enable a wide range of memory optimizations, we demonstrate the benefits of XMem using two use cases: (i) improving the performance portability of software-based cache optimization by expressing the semantics of data locality in the optimization and (ii) improving the performance of OS-based page placement in DRAM by leveraging the semantics of data structures and their access properties.&lt;/p&gt;

&lt;h3 id=&quot;session-3b-coherence--memory-ordering&quot;&gt;Session 3B: Coherence &amp;amp; Memory Ordering&lt;/h3&gt;

&lt;h4 id=&quot;non-speculative-store-coalescing-in-total-store-order&quot;&gt;Non-Speculative Store Coalescing in Total Store Order&lt;/h4&gt;

&lt;h4 id=&quot;dynamic-memory-dependence-predication&quot;&gt;Dynamic Memory Dependence Predication&lt;/h4&gt;

&lt;h4 id=&quot;protogen-automatically-generating-directory-cache-coherence-protocols-from-atomic-specifications&quot;&gt;ProtoGen: Automatically Generating Directory Cache Coherence Protocols from Atomic Specifications&lt;/h4&gt;

&lt;h4 id=&quot;spandex-a-flexible-interface-for-efficient-heterogeneous-coherence&quot;&gt;Spandex: A Flexible Interface for Efficient Heterogeneous Coherence&lt;/h4&gt;

&lt;h3 id=&quot;session-4a-emerging-paradigms&quot;&gt;Session 4A: Emerging Paradigms&lt;/h3&gt;

&lt;h4 id=&quot;flexon-a-flexible-digital-neuron-for-efficient-spiking-neural-network-simulations&quot;&gt;Flexon: A Flexible Digital Neuron for Efficient Spiking Neural Network Simulations&lt;/h4&gt;

&lt;h4 id=&quot;space-time-algebra-a-model-for-neocortical-computation&quot;&gt;Space-Time Algebra: A Model for Neocortical Computation&lt;/h4&gt;

&lt;h4 id=&quot;architecting-a-stochastic-computing-unit-with-molecular-optical-devices&quot;&gt;Architecting a Stochastic Computing Unit with Molecular Optical Devices&lt;/h4&gt;

&lt;h3 id=&quot;session-4b-persistence&quot;&gt;Session 4B: Persistence&lt;/h3&gt;

&lt;h4 id=&quot;density-tradeoffs-of-non-volatile-memory-as-a-replacement-for-sram-based-last-level-cache&quot;&gt;Density Tradeoffs of Non-Volatile Memory as a Replacement for SRAM based Last Level Cache&lt;/h4&gt;

&lt;h4 id=&quot;accord-enabling-associativity-for-gigascale-dram-caches-by-coordinating-way-install-and-way-prediction&quot;&gt;ACCORD: Enabling Associativity for Gigascale DRAM Caches by Coordinating Way-Install and Way-Prediction&lt;/h4&gt;

&lt;h4 id=&quot;rana-towards-efficient-neural-acceleration-with-refresh-optimized-embedded-dram-star&quot;&gt;RANA: Towards Efficient Neural Acceleration with Refresh-Optimized Embedded DRAM :star:&lt;/h4&gt;

&lt;h3 id=&quot;session-5a-emerging-memory-1&quot;&gt;Session 5A: Emerging Memory 1&lt;/h3&gt;

&lt;h4 id=&quot;scaling-datacenter-accelerators-with-compute-reuse-architecture-star&quot;&gt;Scaling Datacenter Accelerators With Compute-Reuse Architecture :star:&lt;/h4&gt;

&lt;p&gt;Hardware specialization is commonly used in data-centers to ameliorate the nearing end of CMOS technology scaling. While offering superior performance and energy-efficiency returns compared to general-purpose processors, specialized accelerators are bound to the same device technology constraints, and are thus prone to similar limitations in the future. Once technology scaling plateaus, accelerator and application tuning will reach a point of near-optimum, with no clear direction for further improvements.&lt;/p&gt;

&lt;p&gt;Emerging non-volatile memory (NVM) technologies follow different scaling trends due to different physical properties and manufacturing techniques. NVMs have inspired recent efforts of innovation in computer systems, as they possess appealing qualities such as high capacity and low energy.&lt;/p&gt;

&lt;p&gt;We present the COmpute-REuse Accelerators (COREx) architecture that shifts computations from the scalability-hindered transistor-based logic towards the continuing-to-scale storage domain. COREx leverages datacenter redundancy by integrating a storage layer together with the accelerator processing layer. The added layer stores the outcomes of previous accelerated computations. The previously computed results are reused in the case of recurring computations, thus eliminating the need to re-compute them.&lt;/p&gt;

&lt;p&gt;We designed COREx as a combination of an accelerator and specialized storage layer using emerging memory technologies, and evaluated it on a set of datacenter workloads. Our results show that, when integrated with a well-tuned accelerator, COREx achieves an average speedup of 6.4X and average savings of 50% in energy and 68% in energy-delay product. We expect further increase in gains in the future, as memory technologies continue to improve steadily.&lt;/p&gt;

&lt;h4 id=&quot;enabling-scientific-computing-on-memristive-accelerators&quot;&gt;Enabling Scientific Computing on Memristive Accelerators&lt;/h4&gt;

&lt;h4 id=&quot;neural-cache-bit-serial-in-cache-acceleration-of-deep-neural-networks-star&quot;&gt;Neural Cache: Bit-Serial In-Cache Acceleration of Deep Neural Networks :star:&lt;/h4&gt;

&lt;p&gt;This paper presents the Neural Cache architecture, which re-purposes cache structures to transform them into massively parallel compute units capable of running inferences for Deep Neural Networks. Techniques to do in-situ arithmetic in SRAM arrays, create efficient data mapping and reducing data movement are proposed. The Neural Cache architecture is capable of fully executing convolutional, fully connected, and pooling layers in-cache. The proposed architecture also supports quantization in-cache. Our experimental results show that the proposed architecture can improve inference latency by 18.3× over state-of-art multi-core CPU (Xeon E5), 7.7× over server class GPU (Titan Xp), for Inception v3 model. Neural Cache improves inference throughput by 12.4× over CPU (2.2× over GPU), while reducing power consumption by 50% over CPU (53% over GPU).&lt;/p&gt;

&lt;h3 id=&quot;session-5b-storage&quot;&gt;Session 5B: Storage&lt;/h3&gt;

&lt;h4 id=&quot;flin-enabling-fairness-and-enhancing-performance-in-modern-nvme-solid-state-drives&quot;&gt;FLIN: Enabling Fairness and Enhancing Performance in Modern NVMe Solid State Drives&lt;/h4&gt;

&lt;h4 id=&quot;grafboost-using-accelerated-flash-storage-for-external-graph-analytics&quot;&gt;GraFBoost: Using accelerated flash storage for external graph analytics&lt;/h4&gt;

&lt;h4 id=&quot;2b-ssd-the-case-for-dual-byte--and-block-addressable-solid-state-drives&quot;&gt;2B-SSD: The Case for Dual, Byte- and Block-Addressable Solid-State Drives&lt;/h4&gt;

&lt;h3 id=&quot;session-6a-emerging-memory-2&quot;&gt;Session 6A: Emerging Memory 2&lt;/h3&gt;

&lt;h4 id=&quot;lazy-persistency-a-high-performing-and-write-efficient-software-persistency-technique&quot;&gt;Lazy Persistency: a High-Performing and Write-Efficient Software Persistency Technique&lt;/h4&gt;

&lt;h4 id=&quot;dhtm-durable-hardware-transactional-memory&quot;&gt;DHTM: Durable Hardware Transactional Memory&lt;/h4&gt;

&lt;h4 id=&quot;hardware-supported-permission-checks-on-persistent-objects-for-performance-and-programmability&quot;&gt;Hardware Supported Permission Checks On Persistent Objects For Performance and Programmability&lt;/h4&gt;

&lt;h3 id=&quot;session-6b-controllers--control-systems&quot;&gt;Session 6B: Controllers &amp;amp; Control Systems&lt;/h3&gt;

&lt;h4 id=&quot;robox-an-end-to-end-solution-to-accelerate-autonomous-control-in-robotics&quot;&gt;RoboX: An End-to-End Solution to Accelerate Autonomous Control in Robotics&lt;/h4&gt;

&lt;h4 id=&quot;dcs-ctrl-a-fast-and-flexible-device-control-mechanism-for-device-centric-server-architecture&quot;&gt;DCS-ctrl: A Fast and Flexible Device-Control Mechanism for Device-Centric Server Architecture&lt;/h4&gt;

&lt;h4 id=&quot;yukta-multilayer-resource-controllers-to-maximize-efficiency&quot;&gt;Yukta: Multilayer Resource Controllers to Maximize Efficiency&lt;/h4&gt;

&lt;h3 id=&quot;session-7a-mobile-platforms&quot;&gt;Session 7A: Mobile Platforms&lt;/h3&gt;

&lt;h4 id=&quot;exploring-predictive-replacement-policies-for-instruction-cache-and-branch-target-buffer&quot;&gt;Exploring Predictive Replacement Policies for Instruction Cache and Branch Target Buffer&lt;/h4&gt;

&lt;h4 id=&quot;eva2-exploiting-temporal-redundancy-in-live-computer-vision&quot;&gt;EVA^2: Exploiting Temporal Redundancy in Live Computer Vision&lt;/h4&gt;

&lt;h4 id=&quot;euphrates-algorithm-soc-co-design-for-low-power-mobile-continuous-vision&quot;&gt;Euphrates: Algorithm-SoC Co-Design for Low-Power Mobile Continuous Vision&lt;/h4&gt;

&lt;h4 id=&quot;guaranteeing-local-differential-privacy-on-ultra-low-power-systems&quot;&gt;Guaranteeing Local Differential Privacy on Ultra-low-power Systems&lt;/h4&gt;

&lt;h4 id=&quot;stitch-fusible-heterogeneous-accelerators-enmeshed-with-many-core-architecture-for-wearables-star&quot;&gt;Stitch: Fusible Heterogeneous Accelerators Enmeshed with Many-Core Architecture for Wearables :star:&lt;/h4&gt;

&lt;p&gt;Wearable devices are now leveraging multi-core processors to cater to the increasing computational demands of the applications via multi-threading. However, the power, performance constraints of many wearable applications can only be satisfied when the thread-level parallelism is coupled with hardware acceleration of common computational kernels. The ASIC accelerators with high performance/watt suffer from high non-recurring engineering costs. Configurable accelerators that can be reused across applications present a promising alternative. Autonomous configurable accelerators loosely-coupled to the processor occupy additional silicon area for local data and control and incur data communication overhead. In contrast, configurable instruction set extension (ISE) accelerators tightly integrated into the processor pipeline eliminate such overheads by sharing the existing processor resources. Yet, naively adding full-blown ISE accelerators to each core in a many-core architecture will lead to huge area and power overheads, which is clearly infeasible in resource-constrained wearables. In this paper, we propose Stitch, a many-core architecture where tiny, heterogeneous, configurable and fusible ISE accelerators, called polymorphic patches are effectively enmeshed with the cores. The novelty of our architecture lies in the ability to stitch together multiple polymorphic patches, where each can handle very simple ISEs, across the chip to create large, virtual accelerators that can execute complex ISEs. The virtual connections are realized efficiently with a very lightweight compiler-scheduled network-on-chip (NoC) with no buffers or control logic. Our evaluations across representative wearable applications show an average 2.3X improvement in runtime for Stitch compared to a baseline many-core processor without ISEs, at a modest area and power overhead.&lt;/p&gt;

&lt;h3 id=&quot;session-7b-security&quot;&gt;Session 7B: Security&lt;/h3&gt;

&lt;h4 id=&quot;nonblocking-memory-refresh&quot;&gt;Nonblocking Memory Refresh&lt;/h4&gt;

&lt;h4 id=&quot;practical-memory-safety-with-rest&quot;&gt;Practical Memory Safety with REST&lt;/h4&gt;

&lt;h4 id=&quot;mitigating-wordline-crosstalk-using-adaptive-trees-of-counters&quot;&gt;Mitigating Wordline Crosstalk using Adaptive Trees of Counters&lt;/h4&gt;

&lt;h4 id=&quot;mobilizing-the-micro-ops-exploiting-context-sensitive-decoding-for-security-and-energy-efficiency&quot;&gt;Mobilizing the Micro-Ops: Exploiting Context Sensitive Decoding for Security and Energy Efficiency&lt;/h4&gt;

&lt;h4 id=&quot;hiding-intermittent-information-leakage-with-architectural-support-for-blinking&quot;&gt;Hiding Intermittent Information Leakage with Architectural Support for Blinking&lt;/h4&gt;

&lt;h3 id=&quot;session-8a-machine-learning-systems-1&quot;&gt;Session 8A: Machine Learning Systems 1&lt;/h3&gt;

&lt;h4 id=&quot;ganax-a-unified-mimd-simd-acceleration-for-generative-adversarial-networks-star&quot;&gt;GANAX: A Unified MIMD-SIMD Acceleration for Generative Adversarial Networks :star:&lt;/h4&gt;

&lt;p&gt;Generative Adversarial Networks (GANs) are one of the most recent deep learning models that generate synthetic data from limited genuine datasets. GANs are on the frontier as further extension of deep learning into many domains (e.g., medicine, robotics, content synthesis) requires massive sets of labeled data that is generally either unavailable or prohibitively costly to collect. Although GANs are gaining prominence in various fields, there are no accelerators for these new models. In fact, GANs leverage a new operator, called transposed convolution, that exposes unique challenges for hardware acceleration. This operator first inserts zeros within the multidimensional input, then convolves a kernel over this expanded array to add information to the embedded zeros. Even though there is a convolution stage in this operator, the inserted zeros lead to underutilization of the compute resources when a conventional convolution accelerator is employed. We propose the GANAX architecture to alleviate the sources of inefficiency associated with the acceleration of GANs using conventional convolution accelerators, making the first GAN accelerator design possible. We propose a reorganization of the output computations to allocate compute rows with similar patterns of zeros to adjacent processing engines, which also avoids inconsequential multiply-adds on the zeros. This compulsory adjacency reclaims data reuse across these neighboring processing engines, which had otherwise diminished due to the inserted zeros. The reordering breaks the full SIMD execution model, which is prominent in convolution accelerators. Therefore, we propose a unified MIMD-SIMD design for GANAX that leverages repeated patterns in the computation to create distinct microprograms that execute concurrently in SIMD mode.&lt;/p&gt;

&lt;h4 id=&quot;snapea-predictive-early-activation-for-reducing-computation-in-deep-convolutional-neural-networks-star&quot;&gt;SnaPEA: Predictive Early Activation for Reducing Computation in Deep Convolutional Neural Networks :star:&lt;/h4&gt;

&lt;p&gt;Deep Convolutional Neural Networks (CNNs) perform billions of operations for classifying a single input. To reduce these computations, this paper offers a solution that leverages a combination of runtime information and the algorithmic structure of CNNs. Specifically, in numerous modern CNNs, the outputs of compute-heavy convolution operations are fed to activation units that output zero if their input is negative. By exploiting this unique algorithmic property, we propose a predictive early activation technique, dubbed SnaPEA. This technique cuts the computation of convolution operations short if it determines that the output will be negative. SnaPEA can operate in two distinct modes, exact and predictive. In the exact mode, with no loss in classification accuracy, SnaPEA statically re-orders the weights based on their signs and periodically performs a single-bit sign check on the partial sum. Once the partial sum drops below zero, the rest of computations can simply be ignored, since the output value will be zero in any case. In the predictive mode, which trades the classification accuracy for larger savings, SnaPEA speculatively cuts the computation short even earlier than the exact mode. To control the accuracy, we develop a multi-variable optimization algorithm that thresholds the degree of speculation. As such, the proposed algorithm exposes a knob to gracefully navigate the trade-offs between the classification accuracy and computation reduction. Compared to a state-of-the-art CNN accelerator, SnaPEA in the exact mode, yields, on average, 28% speedup and 16% energy reduction in various modern CNNs without affecting their classification accuracy. With 3% loss in classification accuracy, on average, 67.8% of the convolutional layers can operate in the predictive mode. The average speedup and energy saving of these layers are 2.02× and 1.89×, respectively. The benefits grow to a maximum of 3.59× speedup and 3.14× energy reduction. Compared to static pruning approaches, which are complimentary to the dynamic approach of SnaPEA, our proposed technique offers up to 63% speedup and 49% energy reduction across the convolution layers with no loss in classification accuracy.&lt;/p&gt;

&lt;h4 id=&quot;ucnn-exploiting-computational-reuse-in-deep-neural-networks-via-weight-repetition-star&quot;&gt;UCNN: Exploiting Computational Reuse in Deep Neural Networks via Weight Repetition :star:&lt;/h4&gt;

&lt;p&gt;Convolutional Neural Networks (CNNs) have begun to permeate all corners of electronic society (from voice recognition to scene generation) due to their high accuracy and machine efficiency per operation. At their core, CNN computations are made up of multi-dimensional dot products between weight and input vectors. This paper studies how weight repetition —when the same weight occurs multiple times in or across weight vectors— can be exploited to save energy and improve performance during CNN inference. This generalizes a popular line of work to improve efficiency from CNN weight sparsity, as reducing computation due to repeated zero weights is a special case of reducing computation due to repeated weights. 
To exploit weight repetition, this paper proposes a new CNN accelerator called the Unique Weight CNN Accelerator (UCNN). UCNN uses weight repetition to reuse CNN sub-computations (e.g., dot products) and to reduce CNN model size when stored in off-chip DRAM —both of which save energy. UCNN further improves performance by exploiting sparsity in weights. We evaluate UCNN with an accelerator-level cycle and energy model and with an RTL implementation of the UCNN processing element. On three contemporary CNNs, UCNN improves throughput-normalized energy consumption by 1.2x - 4x, relative to a similarly provisioned baseline accelerator that uses Eyeriss-style sparsity optimizations. At the same time, the UCNN processing element adds only 17-24% area overhead relative to the same baseline.&lt;/p&gt;

&lt;h4 id=&quot;energy-efficient-neural-network-accelerator-based-on-outlier-aware-low-precision-computation-star&quot;&gt;Energy-Efficient Neural Network Accelerator Based on Outlier-Aware Low-Precision Computation :star:&lt;/h4&gt;

&lt;p&gt;Owing to the presence of large values, which we call outliers, conventional methods of quantization fail to achieve significantly low precision, e.g., four bits, for very deep neural networks, such as ResNet-101. In this study, we propose a hardware accelerator, called the outlier-aware accelerator (OLAccel). It performs dense and low-precision computations for a majority of data (weights and activations) while efficiently handling a small number of sparse and high-precision outliers (e.g., amounting to 3% of total data). The OLAccel is based on 4-bit multiply-accumulate (MAC) units and handles outlier weights and activations in a different manner. For outlier weights, it equips SIMD lanes of MAC units with an additional MAC unit, which helps avoid cycle overhead for the majority of outlier occurrences, i.e., a single occurrence in the SIMD lanes. The OLAccel performs computations using outlier activation on dedicated, high-precision MAC units. In order to avoid coherence problem due to updates from low- and high-precision computation units, both units update partial sums in a pipelined manner. Our experiments show that the OLAccel can reduce by 43.5% (27.0%), 56.7% (36.3%), and 62.2% (49.5%) energy consumption for AlexNet, VGG-16, and ResNet-18, respectively, compared with a 16-bit (8-bit) state-of-the-art zero-aware accelerator. The energy gain mostly comes from the memory components, the DRAM, and on-chip memory due to reduced precision.&lt;/p&gt;

&lt;h3 id=&quot;session-8b-interconnection-networks&quot;&gt;Session 8B: Interconnection Networks&lt;/h3&gt;

&lt;h3 id=&quot;session-9a-machine-learning-systems-2&quot;&gt;Session 9A: Machine Learning Systems 2&lt;/h3&gt;

&lt;h4 id=&quot;prediction-based-execution-on-deep-neural-networks&quot;&gt;Prediction Based Execution on Deep Neural Networks&lt;/h4&gt;

&lt;p&gt;Recently, deep neural network based approaches have emerged as indispensable tools in many fields, ranging from image and video recognition to natural language processing. However, the large size of such newly developed networks poses both throughput and energy challenges to the underlying processing hardware. This could be the major stumbling block to many promising applications such as self-driving cars and smart cities. Existing work proposes to weed zeros from input neurons to avoid unnecessary DNN computation (zero-valued operand multiplications). However, we observe that many output neurons are still ineffectual even if the zero-removal technique has been applied. These ineffectual output neurons could not pass their values to the subsequent layer, which means all the computations (including zero-valued and non-zero-valued operand multiplications) related to these output neurons are futile and wasteful. Therefore, there is an opportunity to significantly improve the performance and efficiency of DNN execution by predicting the ineffectual output neurons and thus completely avoid the futile computations by skipping over these ineffectual output neurons. To do so, we propose a two-stage, prediction-based DNN execution model without accuracy loss. We also propose a uniform serial processing element (USPE), for both prediction and execution stages to improve the flexibility and minimize the area overhead. To improve the processing throughput, we further present a scale-out design for USPE. Evaluation results over a set of state-of-the-art DNNs show that our proposed design achieves 2.5X speedup and 1.9X energy-efficiency on average over the traditional accelerator. Moreover, by stacking with our design, we can improve Cnvlutin and Stripes by 1.9X and 2.0X on average, respectively.&lt;/p&gt;

&lt;h4 id=&quot;bit-fusion-bit-level-dynamically-composable-architecture-for-accelerating-deep-neural-networks&quot;&gt;Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Networks&lt;/h4&gt;

&lt;p&gt;Fully realizing the potential of acceleration for Deep Neural Networks (DNNs) requires understanding and leveraging algorithmic properties. This paper builds upon the algorithmic insight that bitwidth of operations in DNNs can be reduced without compromising their classification accuracy. However, to prevent accuracy loss, the bitwidth varies significantly across DNNs and it may even be adjusted for each layer. Thus, a fixed-bitwidth accelerator would either offer limited benefits to accommodate the worst-case bitwidth requirements, or lead to a degradation in final accuracy. To alleviate these deficiencies, this work introduces dynamic bit-level fusion/decomposition as a new dimension in the design of DNN accelerators. We explore this dimension by designing Bit Fusion, a bit-flexible accelerator, that constitutes an array of bit-level processing elements that dynamically fuse to match the bitwidth of individual DNN layers. This flexibility in the architecture enables minimizing the computation and the communication at the finest granularity possible with no loss in accuracy. We evaluate the benefits of BitFusion using eight real-world feed-forward and recurrent DNNs. The proposed microarchitecture is implemented in Verilog and synthesized in 45 nm technology. Using the synthesis results and cycle accurate simulation, we compare the benefits of Bit Fusion to two state-of-the-art DNN accelerators, Eyeriss and Stripes. In the same area, frequency, and process technology, BitFusion offers 3.9x speedup and 5.1x energy savings over Eyeriss. Compared to Stripes, BitFusion provides 2.6x speedup and 3.9x energy reduction at 45 nm node when BitFusion area and frequency are set to those of Stripes. Scaling to GPU technology node of 16 nm, BitFusion almost matches the performance of a 250-Watt Titan Xp, which uses 8-bit vector instructions, while BitFusion merely consumes 895 milliwatts of power.&lt;/p&gt;

&lt;h4 id=&quot;gist-efficient-data-encoding-for-deep-neural-network-training&quot;&gt;Gist: Efficient Data Encoding for Deep Neural Network Training&lt;/h4&gt;

&lt;p&gt;Modern deep neural networks (DNNs) training typically relies on GPUs to train complex hundred-layer deep networks. A significant problem facing both researchers and industry practitioners is that, as the networks get deeper, the available GPU main memory becomes a primary bottleneck, limiting the size of networks it can train. In this paper, we investigate widely used DNNs and find that the major contributors to memory footprint are intermediate layer outputs (feature maps). We then introduce a framework for DNN-layer-specific optimizations (e.g., convolution, ReLU, pool) that significantly reduce this source of main memory pressure on GPUs. We find that a feature map typically has two uses that are spread far apart temporally. Our key approach is to store an encoded representation of feature maps for this temporal gap and decode this data for use in the backward pass; the full-fidelity feature maps are used in the forward pass and relinquished immediately. Based on this approach, we present Gist, our system that employs two classes of layer-specific encoding schemes – lossless and lossy – to exploit existing value redundancy in DNN training to significantly reduce the memory consumption of targeted feature maps. For example, one insight is by taking advantage of the computational nature of back propagation from pool to ReLU layer, we can store the intermediate feature map using just 1 bit instead of 32 bits per value. We deploy these mechanisms in a state-of-the-art DNN framework (CNTK) and observe that Gist reduces the memory footprint to upto 2× across 5 state-of-the-art image classification DNNs, with an average of 1.8× with only 4% performance overhead. We also show that further software (e.g., CuDNN) and hardware (e.g., dynamic allocation) optimizations can result in even larger footprint reduction (upto 4.1×).&lt;/p&gt;

&lt;h4 id=&quot;the-dark-side-of-dnn-pruning&quot;&gt;The Dark Side of DNN Pruning&lt;/h4&gt;

&lt;h3 id=&quot;session-9b-gpus&quot;&gt;Session 9B: GPUs&lt;/h3&gt;</content><author><name>Zio Lee</name><email>pingziwalk@gmail.com</email></author><category term="ISCA" /><category term="abstract" /><summary type="html">The 45th International Symposium on Computer Architecture</summary></entry><entry><title type="html">Welcome</title><link href="http://localhost:4000/2018/07/01/welcome.html" rel="alternate" type="text/html" title="Welcome" /><published>2018-07-01T00:00:00+08:00</published><updated>2018-07-01T00:00:00+08:00</updated><id>http://localhost:4000/2018/07/01/welcome</id><content type="html" xml:base="http://localhost:4000/2018/07/01/welcome.html">&lt;p&gt;If you see this page, that means you have setup your site. enjoy! :ghost: :ghost: :ghost:&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;You may want to &lt;a href=&quot;https://tianqi.name/jekyll-TeXt-theme/docs/en/configuration&quot;&gt;config the site&lt;/a&gt; or &lt;a href=&quot;https://tianqi.name/jekyll-TeXt-theme/docs/en/writing-posts&quot;&gt;writing a post&lt;/a&gt; next. Please feel free to &lt;a href=&quot;https://github.com/kitian616/jekyll-TeXt-theme/issues&quot;&gt;create an issue&lt;/a&gt; or &lt;a href=&quot;mailto:kitian616@outlook.com&quot;&gt;send me email&lt;/a&gt; if you have any questions.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;If you like TeXt, don’t forget to give me a star :star2:.&lt;/p&gt;

&lt;iframe src=&quot;https://ghbtns.com/github-btn.html?user=kitian616&amp;amp;repo=jekyll-TeXt-theme&amp;amp;type=star&amp;amp;count=true&quot; frameborder=&quot;0&quot; scrolling=&quot;0&quot; width=&quot;170px&quot; height=&quot;20px&quot;&gt;&lt;/iframe&gt;</content><author><name>Zio Lee</name><email>pingziwalk@gmail.com</email></author><category term="TeXt" /><summary type="html">If you see this page, that means you have setup your site. enjoy! :ghost: :ghost: :ghost:</summary></entry></feed>